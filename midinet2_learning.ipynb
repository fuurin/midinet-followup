{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# midinet2モデルの学習\n",
    "midinet2は楽曲のより詳細な情報を条件として受け入れられるようにしたmidinetベースの音楽生成GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## midinet2 最終的な工夫のまとめ\n",
    "モデルの大胆な変更は行わず，小手先のテクニックでどこまでうまくいけるかを試した  \n",
    "聴ける曲を作るには，モデルや学習手法よりも前処理が肝要であった．  \n",
    "最終的に，ただのmidinetよりはメロディらしさが増し，人が作るようなフレーズも散見されるようになった．\n",
    "\n",
    "### 前処理\n",
    "- 全体的なこと\n",
    "    - スケール外の音を学習しないよう，トランスポーズによる水増しを行わない\n",
    "        - これによりスケール外の音を使用することはほとんどなくなった\n",
    "- メロディのこと\n",
    "    - 1拍のステップ数を4->12とし，3連符や音符の切れ目にも対応\n",
    "        - 実際，長く学習を進めれば音符の切れ目を作ってくれるようになった\n",
    "    - 最もメロディに合致するkeyを推定し，key=Cへトランスポーズ\n",
    "        - データセットで提示されているスケールは合っていないことがあった\n",
    "        - これによりスケール外の音を使用することはほとんどなくなった\n",
    "    - MIDI音程をデータの9割方が含まれる64音程に限定\n",
    "    - さらに2オクターブ内に音程を押し込めた\n",
    "        - これにより極端な音飛びが発生しにくくなった\n",
    "    - 「何も鳴らさなければ偽物とバレない」をさせないため9割以上が音符を含む小節のみを学習\n",
    "        - これにより学習を長く進めても空の小節を作ることはほとんどなくなった\n",
    "- コードのこと\n",
    "    - 1小節に複数のコードがあることに対応するため，拍ごとの先頭コード4つを連結し48次元に\n",
    "    - ここでそれぞれの拍はコードのルートではなく，構成音をそのままベクトルに使用\n",
    "        - 効果があったのかはよくわからないが，構成音を使う時の方が多い気もする\n",
    "- セクションのこと\n",
    "    - コードベクトルに6次元のセクション情報を加えた．\n",
    "        - これはあまり効果があったようには思えない\n",
    "\n",
    "### モデル(midinet2)\n",
    "- G,Dともにカーネルサイズ(3,1)の畳み込み/逆畳み込み層を追加\n",
    "    - 1拍4×3=12ステップの小節データに対応\n",
    "    - 学習が甘いと細かい音符を拡散させてしまうが，学習を進めれば鳴らす音符は1つに定まってくる\n",
    "- コンストラクタでノイズベクトルのサイズを受け取る\n",
    "    - モデルのパラメータ数を初期化時に決められる\n",
    "    - 条件ベクトルが5倍くらいになったので，ノイズベクトルも512と約5倍にした\n",
    "\n",
    "### 学習方法\n",
    "- Noisy Label: Dの学習時ラベルを乱数で求めることでDが強くなりすぎたり過学習するのを防ぐ\n",
    "    - 本物データに対しては0.7～1.2, 偽物データに対しては0～0.3\n",
    "- feature mappingによるペナルティが軽すぎたのでせめて$\\lambda_1=0.1, \\lambda_2=0.5$と大きくした\n",
    "    - そもそもfeature mappingがうまく働いているのかわからない\n",
    "- Gの学習時，ノイズベクトルは毎回新しいものを使用\n",
    "    - 経験的に明らかに結果が良くなった\n",
    "- Dの学習1回に対するGの学習回数は，「本物を本物と見分けられる優秀なDに偽物だと見抜かれるならGはその分がんばる」という方法で回数を動的に決定\n",
    "    - 具体的には，(realD - fakeD) × 100 × k (kは大きいほど回数を増やすHP)\n",
    "    - Dの学習が進んでないうちにGが置いてけぼりにしたり，Dに勝てなくなったりすることが少なくなった\n",
    "    - 学習後半では結局Dに段々負けていってしまっているが，結果の曲は結構いい\n",
    "\n",
    "### 後処理\n",
    "- ステップごとの最大値のみを鳴らすmonophonize(これは本家もやっていた)\n",
    "- ステップ内の出力結果の値が全て閾値を越えなければ休符とみなす\n",
    "    - 学習が進んだモデルはほとんど出力結果が1と0に分かれるのであまり必要でない\n",
    "    - 出力マップを直接imshowで観察するshow_mapでも精度が良い(鳴らす音符がほとんど一つに定まる)ことが確認されている\n",
    "- 16分音符などのちょうどいい大きさの音符にDownResolutionするfill\n",
    "    - 細かい音符を消し去るのが目的\n",
    "    - その新しいステップの終端が空白なら音符の切れ目と判断して削る\n",
    "- 細かい音符を直接消し去るremove_short_notes\n",
    "    - 学習が進んだモデルを使えばfillだけで十分\n",
    "- 学習の条件となる譜面を作成するだけでなく，トランスポーズやオーディオ化，視聴，MIDI化が簡単にできるNotebookを作成\n",
    "\n",
    "### まとめと展望\n",
    "上記のことをやったが，やはり前の小節一つのみを条件に使うのでは，フレーズの繰り返しなどを表現することができず，落ち着かない感じになってしまう．  \n",
    "以降では，次のような手法を用い，モデルそのものの改良を行っていく\n",
    "- より長大なデータを入力できると期待されるSelf-Attention\n",
    "- ランダムノイズが少なくてもモード崩壊を防げるWGAN, WGAN-gpの導入\n",
    "  \n",
    "  \n",
    "- ていうか，DにPrev入れないで判定させるの間違ってるのでは？\n",
    "- on chord rate が有意に落ちた．．．もう2次元条件を2つ使えばいいのでは？\n",
    "- 条件付けはテンポでやったほうがいいのでは？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, ipdb, pickle, numpy as np\n",
    "visible_devices = [3]\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ','.join([str(i) for i in visible_devices])\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from utils import Timer, get_model\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"../datasets/theorytab/midinet\"\n",
    "output_dir = f\"{base_dir}/learning\"\n",
    "\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaderの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MidinetDataloader():\n",
    "    def __init__(self, data_path, pitch_range=[0, 64], show_shape=False):\n",
    "        data = pickle.load(open(data_path,'rb'))\n",
    "        \n",
    "        melody, prev, chord = [], [], []\n",
    "        for m, p, c in data:\n",
    "            melody.append(m)\n",
    "            prev.append(p)\n",
    "            chord.append(c)\n",
    "        \n",
    "        self.size = len(melody)\n",
    "        steps = len(melody[0])\n",
    "        bottom, top = pitch_range\n",
    "        \n",
    "        melody = np.array(melody)[:,:,bottom:top].reshape(self.size, 1, steps, top-bottom)\n",
    "        prev = np.array(prev)[:,:,bottom:top].reshape(self.size, 1, steps, top-bottom)\n",
    "        chord = np.array(chord)\n",
    "        \n",
    "        if show_shape:\n",
    "            print(\"melody shape\", melody.shape)\n",
    "            print(\"prev shape\", prev.shape)\n",
    "            print(\"condition shape\", chord.shape)\n",
    "        \n",
    "        self.x = torch.from_numpy(melody).float()\n",
    "        self.prev_x   = torch.from_numpy(prev).float()\n",
    "        self.y  = torch.from_numpy(chord).float()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.prev_x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(data_path, batch_size=72, shuffle=True):\n",
    "    iterator = MidinetDataloader(data_path)\n",
    "    kwargs = {'num_workers': 4, 'pin_memory': True}\n",
    "    data_loader = DataLoader(iterator, batch_size=batch_size, shuffle=shuffle, **kwargs)\n",
    "    print('Data loading is completed.')\n",
    "    print(f'{len(data_loader)} batches from {len(iterator)} bars are obtained.')\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(data):\n",
    "    if not isinstance(data, torch.Tensor):\n",
    "        data = torch.from_numpy(data)\n",
    "    if torch.cuda.is_available():\n",
    "        return data.cuda()\n",
    "    return data.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model用共通関数の作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_vector(x, y):\n",
    "    x_0, _, x_2, x_3 = x.shape\n",
    "    y2 = y.expand(x_0, y.shape[1], x_2, x_3)\n",
    "    return torch.cat((x, y2),1)\n",
    "    \n",
    "def batch_norm(x, eps=1e-05, momentum=0.9, affine=True):\n",
    "    if x.ndim == 2:\n",
    "        return nn.BatchNorm1d(x.shape[1], eps=eps, momentum=momentum, affine=affine).cuda()(x)\n",
    "    elif x.ndim == 3:\n",
    "        return nn.BatchNorm2d(x.shape[1], eps=eps, momentum=momentum, affine=affine).cuda()(x)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def lrelu(x, leak=0.2):\n",
    "    z = torch.mul(x,leak)\n",
    "    return torch.max(x, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator\n",
    "ここからが本題.モデルの入力を変えたい  \n",
    "\n",
    "forwardの入力\n",
    "- z (batch, noise_size) = (72, 100): ランダムノイズ\n",
    "- prev_x (batch, ch, steps, pitch) = (72, 1, 48, 64): 前の小節\n",
    "- condition (batch, 54): 最初の12次元×4は拍ごとの和音, 最後の6次元はセクション\n",
    "\n",
    "forwardの出力\n",
    "- g_x (batch, ch, steps, pitch)= (72, 1, 48, 64): 生成された今の小節"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "畳み込みを1段階増やす  \n",
    "おそらく三連譜が出てくるのは1/12分音符として使われるのが一般的だろうと踏んで，12->4のところでカーネルサイズを変更  \n",
    "prev_x -> ピッチ畳み込み -> 48 -> 24 -> 12 -> 4 -> 2 -> 1  \n",
    "ノイズ増やした方がいいかな？  \n",
    "あとなんでストライドが縦に2なのか？最初にpitch方向は1に潰してるから関係ないと思うけど"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz=100, pitch_range=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.nz      = nz\n",
    "        self.y_dim   = 54\n",
    "        \n",
    "        self.prev_ch = 16\n",
    "        self.gf_dim  = 128\n",
    "        self.gfc_dim = 1024\n",
    "\n",
    "        self.h1      = nn.ConvTranspose2d(in_channels=self.gf_dim+self.y_dim+self.prev_ch, out_channels=pitch_range, kernel_size=(2,1), stride=(2,2))\n",
    "        self.h2      = nn.ConvTranspose2d(in_channels=pitch_range+self.y_dim+self.prev_ch, out_channels=pitch_range, kernel_size=(3,1), stride=(3,2))\n",
    "        self.h3      = nn.ConvTranspose2d(in_channels=pitch_range+self.y_dim+self.prev_ch, out_channels=pitch_range, kernel_size=(2,1), stride=(2,2))\n",
    "        self.h4      = nn.ConvTranspose2d(in_channels=pitch_range+self.y_dim+self.prev_ch, out_channels=pitch_range, kernel_size=(2,1), stride=(2,2))\n",
    "        self.h5      = nn.ConvTranspose2d(in_channels=pitch_range+self.y_dim+self.prev_ch, out_channels=1, kernel_size=(1,pitch_range), stride=(1,2))\n",
    "\n",
    "        self.h0_prev = nn.Conv2d(in_channels=1, out_channels=self.prev_ch, kernel_size=(1,pitch_range), stride=(1,2))\n",
    "        self.h1_prev = nn.Conv2d(in_channels=self.prev_ch, out_channels=self.prev_ch, kernel_size=(2,1), stride=(2,2))\n",
    "        self.h2_prev = nn.Conv2d(in_channels=self.prev_ch, out_channels=self.prev_ch, kernel_size=(2,1), stride=(2,2))\n",
    "        self.h3_prev = nn.Conv2d(in_channels=self.prev_ch, out_channels=self.prev_ch, kernel_size=(3,1), stride=(3,2))\n",
    "        self.h4_prev = nn.Conv2d(in_channels=self.prev_ch, out_channels=self.prev_ch, kernel_size=(2,1), stride=(2,2))\n",
    "\n",
    "        self.linear1 = nn.Linear(self.nz + self.y_dim, self.gfc_dim)\n",
    "        self.linear2 = nn.Linear(self.gfc_dim + self.y_dim, self.gf_dim*2*1)\n",
    "\n",
    "    def forward(self, z, prev_x, y, batch_size):\n",
    "        \n",
    "        h0_prev = lrelu(batch_norm(self.h0_prev(prev_x)))   # 72, 16, 48, 1 ピッチ方向の畳み込み\n",
    "        h1_prev = lrelu(batch_norm(self.h1_prev(h0_prev)))  # 72, 16, 24, 1\n",
    "        h2_prev = lrelu(batch_norm(self.h2_prev(h1_prev)))  # 72, 16, 12, 1\n",
    "        h3_prev = lrelu(batch_norm(self.h3_prev(h2_prev)))  # 72, 16, 4, 1\n",
    "        h4_prev = lrelu(batch_norm(self.h4_prev(h3_prev)))  # 72, 16, 2, 1\n",
    "\n",
    "        yb = y.view(batch_size, self.y_dim, 1, 1)           # 72, 54, 1, 1\n",
    "\n",
    "        z = torch.cat((z,y),1)                              # 72, 154\n",
    "\n",
    "        h0 = F.relu(batch_norm(self.linear1(z)))            # 72, 1024\n",
    "        h0 = torch.cat((h0,y),1)                            # 72, 1078\n",
    "\n",
    "        h1 = F.relu(batch_norm(self.linear2(h0)))           # 72, 256\n",
    "        h1 = h1.view(batch_size, self.gf_dim, 2, 1)         # 72, 128, 2, 1\n",
    "        \n",
    "        h1 = concat_vector(h1, yb)                          # 72, 182, 2, 1\n",
    "        h1 = concat_vector(h1, h4_prev)                     # 72, 198, 2, 1\n",
    "\n",
    "        h2 = F.relu(batch_norm(self.h1(h1)))                # 72,  64, 4, 1 逆畳み込み． チャンネル数は固定で，3次元目が拡大される\n",
    "        h2 = concat_vector(h2, yb)                          # 72, 118, 4, 1\n",
    "        h2 = concat_vector(h2, h3_prev)                     # 72, 134, 4, 1\n",
    "\n",
    "        h3 = F.relu(batch_norm(self.h2(h2)))                # 72,  64, 12, 1 \n",
    "        h3 = concat_vector(h3, yb)                          # 72, 118, 12, 1\n",
    "        h3 = concat_vector(h3, h2_prev)                     # 72, 134, 12, 1\n",
    "\n",
    "        h4 = F.relu(batch_norm(self.h3(h3)))                # 72,  64, 24, 1\n",
    "        h4 = concat_vector(h4, yb)                          # 72, 118, 24, 1\n",
    "        h4 = concat_vector(h4, h1_prev)                     # 72, 134, 24, 1\n",
    "        \n",
    "        h5 = F.relu(batch_norm(self.h4(h4)))                # 72,  64, 48, 1\n",
    "        h5 = concat_vector(h5, yb)                          # 72, 118, 48, 1\n",
    "        h5 = concat_vector(h5, h0_prev)                     # 72, 134, 48, 1\n",
    "\n",
    "        g_x = torch.sigmoid(self.h5(h5))                    # 72, 1, 48, 64  ピッチ方向の逆畳み込み\n",
    "\n",
    "        return g_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator\n",
    "forwardの入力\n",
    "- x (batch, 1, steps, pitch) = (72, 1, 48, 64): real/fake判定を行う小節データ\n",
    "- y (batch, 54) = (72, 54): コード＋セクションの条件データ\n",
    "\n",
    "forwardの出力\n",
    "- h4_sigmoid (batch, 1) = (72, 1): 0~1に押し込められたreal/fake判定結果．0はfake, 1はreal\n",
    "- h4 (batch, 1) = (72, 1): 0~1に押し込められていないreal/fake判定結果\n",
    "- fm (batch, 1+13, steps, pitch) = (72, 109, 8, 1): 特徴マップ．\n",
    "\n",
    "こちらも層を1つ追加  \n",
    "思ったんだけどprev_xは識別に使わなくていいの？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, nz=100, pitch_range=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.y_dim = 54\n",
    "\n",
    "        self.df_dim = 64\n",
    "        self.dfc_dim = 1024\n",
    "        \n",
    "        self.h0_prev = nn.Conv2d(\n",
    "            in_channels=self.y_dim + 1, \n",
    "            out_channels=self.y_dim + 1, \n",
    "            kernel_size=(2,pitch_range), \n",
    "            stride=(2,2)\n",
    "        )\n",
    "        \n",
    "        self.h1_prev = nn.Conv2d(\n",
    "            in_channels=self.y_dim * 2 + 1, \n",
    "            out_channels=self.y_dim * 2 + 1, \n",
    "            kernel_size=(3,1), \n",
    "            stride=(3,2)\n",
    "        )\n",
    "        \n",
    "        self.h2_prev = nn.Conv2d(\n",
    "            in_channels=self.y_dim * 3 + 1,\n",
    "            out_channels=self.df_dim + self.y_dim, \n",
    "            kernel_size=(4,1), \n",
    "            stride=(2,2)\n",
    "        )\n",
    "        \n",
    "        self.linear1 = nn.Linear((self.df_dim + self.y_dim) * 3 + self.y_dim, self.dfc_dim)\n",
    "        self.linear2 = nn.Linear(self.dfc_dim + self.y_dim, 1)\n",
    "\n",
    "    def forward(self, x, y, batch_size):        \n",
    "\n",
    "        yb = y.view(batch_size,self.y_dim, 1, 1)\n",
    "        x = concat_vector(x, yb)                    # 72, 55, 48, 64\n",
    "        \n",
    "        h0 = lrelu(self.h0_prev(x))                 # 72,  55, 24, 1\n",
    "        h0 = concat_vector(h0, yb)                  # 72, 109, 24, 1\n",
    "\n",
    "        h1 = lrelu(self.h1_prev(h0))                # 72, 109, 8, 1\n",
    "        fm = h1\n",
    "        h1 = concat_vector(h1, yb)                  # 72, 163, 8, 1\n",
    "        \n",
    "        h2 = lrelu(batch_norm(self.h2_prev(h1)))    # 72, 118, 3, 1 8マス上で4マスのストライド2は3になる\n",
    "        h2 = h2.view(batch_size, -1)                # 72, 354\n",
    "        h2 = torch.cat((h2,y), 1)                   # 72, 408\n",
    "\n",
    "        h3 = lrelu(batch_norm(self.linear1(h2)))    # 72, 1024\n",
    "        h3 = torch.cat((h3,y), 1)                   # 72, 1078\n",
    "\n",
    "        h4 = self.linear2(h3)                       # 72, 1\n",
    "        h4_sigmoid = torch.sigmoid(h4)              # 72, 1\n",
    "\n",
    "\n",
    "        return h4_sigmoid, h4, fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'midinet2_dense'\n",
    "input_data_path = os.path.join(base_dir, \"midinet2_dense.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(output_dir, version)\n",
    "\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "save_npa = lambda file_name, npa: np.save(os.path.join(save_dir, file_name), npa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ハイパーパラメータの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = None  # 学習再開用．初回学習時にはNoneに設定すること\n",
    "\n",
    "epochs = 500\n",
    "save_frequency = 10  # epocks // save_frequency ごとにモデルを保存\n",
    "batch_size = 1600\n",
    "sample_bar_num = 8  # 学習過程で保存する画像の小節数\n",
    "\n",
    "# generator_train_times = 16\n",
    "k = 4\n",
    "\n",
    "# for Adam\n",
    "lr = 0.0002\n",
    "betas = (0.9, 0.999)\n",
    "\n",
    "# noise vector size\n",
    "nz = 512\n",
    "\n",
    "# feature matching coefficients\n",
    "lambda_1, lambda_2 = 0.1, 0.5 # D, G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習初期化処理  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading is completed.\n",
      "54 batches from 85927 bars are obtained.\n"
     ]
    }
   ],
   "source": [
    "data_loader = get_dataloader(input_data_path, batch_size=batch_size)\n",
    "data_size = len(data_loader.dataset) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New models are created!\n"
     ]
    }
   ],
   "source": [
    "if start_epoch is None:\n",
    "    netD = Discriminator()\n",
    "    netG = Generator(nz=nz)\n",
    "    print(\"New models are created!\")\n",
    "else:\n",
    "    netD = get_model(save_dir, Discriminator, prefix=f\"netD_epoch={start_epoch:04}\")\n",
    "    netG = get_model(save_dir, Generator, prefix=f\"netG_epoch={start_epoch:04}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    netD = netD.cuda()\n",
    "    netG = netG.cuda()\n",
    "\n",
    "netD.train()\n",
    "netG.train()\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=betas)\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "noise_for_sample = to_device(torch.randn(sample_bar_num, nz))\n",
    "\n",
    "realD_list, fakeD_list = [], [] # Dのrealデータとfakeデータに対するエポックごとの識別結果平均(realに対しては1に近く，fakeに対しては0に近い方がDが強い)\n",
    "lossD_list, lossG_list = [], [] # D, Gのロスのエポックごとの誤差\n",
    "gtt_list = [] # 動的に変わるエポックごとのGの学習回数\n",
    "saved_image_paths = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習ループ\n",
    "オリジナルのコードを若干書き換え  \n",
    "ノイズベクトルは毎回作り直すことにしてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start leaning...\")\n",
    "\n",
    "def save_model(generator, discriminator, epoch, gtt):\n",
    "    hyper_param_str = f\"epoch={epoch:04}_nz={nz}_l1={lambda_1}_l2={lambda_2}_gt={gtt}\"\n",
    "    torch.save(netG.state_dict(), os.path.join(save_dir, f'netG_{hyper_param_str}.pth'))\n",
    "    torch.save(netD.state_dict(), os.path.join(save_dir, f'netD_{hyper_param_str}.pth'))\n",
    "\n",
    "if start_epoch is None:\n",
    "    start_epoch = 0\n",
    "else:\n",
    "    realD_list = np.load(os.path.join(save_dir, 'realD_list.npy')).tolist()\n",
    "    fakeD_list = np.load(os.path.join(save_dir, 'fakeD_list.npy')).tolist()\n",
    "    lossD_list = np.load(os.path.join(save_dir, 'lossD_list.npy')).tolist()\n",
    "    lossG_list = np.load(os.path.join(save_dir, 'lossG_list.npy')).tolist()\n",
    "    gtt_list = np.load(os.path.join(save_dir, 'gtt_list.npy')).tolist()\n",
    "    \n",
    "\n",
    "generator_train_times = k\n",
    "for epoch in range(start_epoch+1, epochs+1):\n",
    "    sum_lossD, sum_lossG  = 0, 0\n",
    "    sum_realD, sum_fakeD  = 0, 0\n",
    "    \n",
    "    with Timer():\n",
    "        for i, (real, prev, chord) in enumerate(data_loader):\n",
    "\n",
    "            # バッチ(譜面，前の譜面，コード)をdeviceに渡す  \n",
    "            real, prev, chord = [to_device(item) for item in [real, prev, chord]]\n",
    "\n",
    "            # batchの切れ端はサイズが異なる場合があるので注意\n",
    "            batch_size = real.size(0)\n",
    "\n",
    "            ############################\n",
    "            # Dの学習: log(D(x)) + log(1 - D(G(z))) を最大化\n",
    "            ###########################\n",
    "\n",
    "            # Dの勾配の初期化\n",
    "            netD.zero_grad()\n",
    "\n",
    "            # realに対する識別結果からクロスエントロピー誤差(目的関数)の値を得る\n",
    "            d_real, d_logits_real, fm_real = netD(real, chord, batch_size)\n",
    "            d_real_label = torch.rand_like(d_logits_real) * 0.5 + 0.7\n",
    "            d_loss_real = nn.BCEWithLogitsLoss()(d_logits_real, d_real_label)\n",
    "\n",
    "            # Gにノイズベクトル，前の譜面，コードを渡し，fakeデータを作成\n",
    "            noise = to_device(torch.randn(batch_size, nz))\n",
    "            fake = netG(noise, prev, chord, batch_size)\n",
    "\n",
    "            # fakeに対する識別結果からクロスエントロピー誤差(目的関数)の値を得る\n",
    "            d_fake, d_logits_fake, fm_fake = netD(fake.detach(), chord, batch_size)\n",
    "            d_fake_label = torch.rand_like(d_logits_fake) * 0.3\n",
    "            d_loss_fake = nn.BCEWithLogitsLoss()(d_logits_fake, d_fake_label)\n",
    "\n",
    "            # 誤差逆伝搬により勾配を更新し，それに基づきDのパラメータを更新する\n",
    "            lossD = d_loss_real + d_loss_fake\n",
    "            lossD.backward(retain_graph=True)\n",
    "            optimizerD.step()\n",
    "\n",
    "            # 学習記録\n",
    "            # real, fakeデータに対してそれぞれrealだと識別した割合\n",
    "            realD, fakeD = d_real.mean().item(), d_fake.mean().item()\n",
    "            sum_realD += realD\n",
    "            sum_fakeD += fakeD\n",
    "            sum_lossD += lossD.item() # Dの学習におけるLoss\n",
    "\n",
    "\n",
    "            ############################\n",
    "            # Gの学習 : log(D(G(z)))を最大化\n",
    "            ###########################\n",
    "\n",
    "            for t in range(generator_train_times):\n",
    "\n",
    "                # Gの勾配の初期化\n",
    "                netG.zero_grad()\n",
    "\n",
    "                # Gにノイズベクトル，前の譜面，コードを渡し，fakeデータを作成 (やっぱりいちいち作ったほうが結果がよさそう)\n",
    "                noise = to_device(torch.randn(batch_size, nz))\n",
    "                fake = netG(noise, prev, chord, batch_size)\n",
    "                \n",
    "                # fakeに対して1をラベルとした識別結果からクロスエントロピー誤差(目的関数)の値を得てGの誤差とする\n",
    "                d_fake, d_logits_fake, fm_fake = netD(fake, chord, batch_size)\n",
    "                deceive_label = torch.ones_like(d_logits_fake)\n",
    "                g_loss = nn.BCEWithLogitsLoss()(d_logits_fake, deceive_label) # (72, 1), (72, 1) => scalar tensor\n",
    "\n",
    "                # Dの特徴マッチング：realとfakeでnetDの初段のreluの出力が近くなるようにする\n",
    "                features_from_g = torch.mean(fm_fake, 0) # fakeデータに対するDのfeatureの平均値\n",
    "                features_from_i = torch.mean(fm_real, 0) # realデータに対するDのfeatureの平均値\n",
    "                # fakeとrealの出すfeatureの違いが大きいほどペナルティを与える\n",
    "                fm_g_loss1 = nn.MSELoss(reduction='sum')(features_from_g, features_from_i) / 2\n",
    "                fm_g_loss1 = torch.mul(fm_g_loss1, lambda_1)\n",
    "\n",
    "                # Gの特徴マッチング：Gがrealに近いデータを生成できるようにする\n",
    "                mean_image_from_g = torch.mean(fake, 0) # fakeデータの平均値\n",
    "                mean_image_from_i = torch.mean(real, 0) # realデータの平均値\n",
    "                # fakeデータとrealデータの違いが大きいほどペナルティを与える\n",
    "                fm_g_loss2 = nn.MSELoss(reduction='sum')(mean_image_from_g, mean_image_from_i) / 2\n",
    "                fm_g_loss2 = torch.mul(fm_g_loss2, lambda_2)\n",
    "\n",
    "                # 誤差逆伝搬により勾配を更新し，それに基づきGのパラメータを更新する\n",
    "                lossG = g_loss + fm_g_loss1 + fm_g_loss2\n",
    "                lossG.backward(retain_graph=(t < generator_train_times - 1)) # 最後は計算グラフを放棄\n",
    "                optimizerG.step()\n",
    "\n",
    "            # 学習記録\n",
    "            sum_lossG += lossG.item() # Gの学習におけるLoss\n",
    "\n",
    "        clear_output()\n",
    "        print(f\"epoch {epoch} / {epochs} result\")\n",
    "        \n",
    "        # エポックごとの識別と誤差の記録\n",
    "        realD_list.append(sum_realD / data_size)\n",
    "        fakeD_list.append(sum_fakeD / data_size)\n",
    "        lossD_list.append(sum_lossD / data_size)\n",
    "        lossG_list.append(sum_lossG / data_size)\n",
    "        gtt_list.append(generator_train_times)\n",
    "        \n",
    "        # adaptive gtt: 本物を本物と識別できる奴に偽物だと見破られるほど生成器が頑張る\n",
    "        avg_realD = sum_realD / data_size\n",
    "        avg_fakeD = sum_fakeD / data_size\n",
    "        generator_train_times = min(48, max(1, int((avg_realD - avg_fakeD) * 100)) * k)\n",
    "        print(f\"==> next generator train times: {generator_train_times}\")\n",
    "        \n",
    "        print(f'==> avg lossD: {lossD_list[-1]:.4f} avg lossG: {lossG_list[-1]:.4f}, avg realD: {realD_list[-1]:.4f}, avg fakeD: {fakeD_list[-1]:.4f} ')\n",
    "        print(f'==> last values[ loss D: {lossD:.4f} loss G: {lossG:.4f} = {g_loss:.4f} + {fm_g_loss1:.4f} + {fm_g_loss2:.4f} real D: {realD:.4f} fake D: {fakeD:.4f} ]')\n",
    "            \n",
    "    # epochs/save_frequency ごとにモデルとここまでの誤差リストを保存し，生成データを画像で記録\n",
    "    if epoch % (epochs // save_frequency) == 0:\n",
    "        save_model(netG, netD, epoch, generator_train_times)\n",
    "        \n",
    "        sample_fake = netG(noise_for_sample, prev[:sample_bar_num], chord[:sample_bar_num], sample_bar_num).detach()\n",
    "        _, _, steps, pitch_range = sample_fake.shape\n",
    "        sample_fake = sample_fake.reshape(sample_bar_num*steps, pitch_range).T\n",
    "\n",
    "        fake_image_path = os.path.join(save_dir, f'fake_samples_epoch{epoch:03}.png')\n",
    "        saved_image_paths.append(fake_image_path)\n",
    "        vutils.save_image(sample_fake, fake_image_path, normalize=True)\n",
    "\n",
    "        save_npa('realD_list.npy', realD_list)\n",
    "        save_npa('fakeD_list.npy', fakeD_list)\n",
    "        save_npa('lossD_list.npy', lossD_list)\n",
    "        save_npa('lossG_list.npy', lossG_list)\n",
    "        save_npa('gtt_list.npy', gtt_list)\n",
    "\n",
    "print(\"finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成データの画像を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = glob.glob(os.path.join(save_dir, \"fake_samples_*\"))\n",
    "image_paths.sort()\n",
    "\n",
    "print(f\"{len(image_paths)} images found in {save_dir}\")\n",
    "for i, path in enumerate(image_paths):\n",
    "    print(f\"{i}: {path.split('/')[-1]}\")\n",
    "\n",
    "image_path = image_paths[int(input(\"input the number of image:\"))]\n",
    "\n",
    "img = np.array(Image.open(image_path))\n",
    "fig, ax = plt.subplots(figsize=(21, 7))\n",
    "ax.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 誤差グラフの表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burn_in = 10\n",
    "\n",
    "lossD_print = np.load(os.path.join(save_dir, 'lossD_list.npy'))[burn_in:]\n",
    "lossG_print = np.load(os.path.join(save_dir, 'lossG_list.npy'))[burn_in:]\n",
    "gtt_print = np.load(os.path.join(save_dir, 'gtt_list.npy'))[burn_in:]\n",
    "\n",
    "length = lossG_print.shape[0]\n",
    "length = 100\n",
    "x = np.asarray(np.linspace(burn_in, burn_in + length-1, length))\n",
    "fig, axs = plt.subplots(2, 1)\n",
    "axs[0].plot(x, lossD_print[:length],label=' lossD', linewidth=1.5)\n",
    "axs[0].plot(x, lossG_print[:length],label=' lossG', linewidth=1.5)\n",
    "axs[0].legend(loc='upper right')\n",
    "axs[0].set_xlabel('epoch')\n",
    "axs[0].set_ylabel('loss')\n",
    "\n",
    "axs[1].plot(x, gtt_print[:length], label=' gtt', linewidth=1.5)\n",
    "axs[1].legend(loc='upper right')\n",
    "axs[1].set_xlabel('epoch')\n",
    "axs[1].set_ylabel('gtt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "midinet2\n",
    "- 今のところ遊べるやつ\n",
    "    - midinet2_dence/netG_epoch=150_nx=512_l1=1_l2=5_gt=24.pth\n",
    "    - midinet2_dence/netG_epoch=0500_nz=512_l1=0.1_l2=0.5_gt=48.pth\n",
    "- 1:10で学習させてもまだDが強いので，新たな[テクニック](https://qiita.com/underfitting/items/a0cbb035568dea33b2d7)を導入\n",
    "    - midinet2_hacked\n",
    "        - Gのforwardすべてでlreluを使用\n",
    "        - DのラベルをSoftNoisyなものに変更(fakeのラベルを[0.0, 0.3], realのラベルを[0.7, 1.2]で一様分布から取得)\n",
    "        - ノイズを256に増やす(条件ベクトルの10倍くらいのものが使われることが多そうなので512でもいいくらい?)\n",
    "        - AdamのBeta1を0.9にして学習を加速(?)\n",
    "        - ノイズを512，学習回数1:16で学習させてみる\n",
    "            - 学習の記録はGのLossが段々増加，DのLossが段々現象．逆になってほしい…\n",
    "            - しかし分散は小さかったので，いい線は言っていると思う\n",
    "            - 生成される音楽は\n",
    "                - 学習が進むほど空白が多い\n",
    "                - 鳴っているところも細かい音符が多くて聞き心地が悪い\n",
    "                - うまくいっている小節はメロディらしくなっているが，あまりうまくいっているものはない\n",
    "                - 条件にも特に則っているようには思えない\n",
    "                - epoch200になるともはや何も生成しない\n",
    "            - とにかく空白が多いのがよくない．\n",
    "            - 小節の1/3以上が鳴っていない楽譜を削除してみよう\n",
    "            - 全てlreluにしたのは悪手だったかも．出力が消える．\n",
    "    - midinet2_dense\n",
    "        - 9割がon_noteである小節のみを採用したmidinet2_denseデータセットを作成して使用\n",
    "        - lreluにしたところをF.leruに直した\n",
    "        - もうちょいGに頑張ってほしいので1:24にしてみた\n",
    "        - これは遊べる！！\n",
    "            - 学習の過程も良い感じ(グラフ途中で消えちゃったけどね)\n",
    "            - fillあるとめっちゃ自然になるけど，なくても結構いけるレベル\n",
    "            - epochが大きくなるとCで終わってくれなくなる.150epoch辺りが聴き心地良い\n",
    "    - midinet2_dense-2\n",
    "        - ちゃんとしたデータが欲しかったので試しに$\\lambda_1 = 0.1, \\lambda_2 = 0.5$にして再挑戦\n",
    "        - どの音が使われているかわかればいいので，2オクターブに押し込めるのアリな気がしてきた  \n",
    "            - G3~F#5で試してみる\n",
    "            - これは音飛びを減少させるのに一役買ったっぽい\n",
    "        - adaptive gtt(勝手に命名)：本物を本物と識別できる奴に偽物だと見破られるほど頑張る\n",
    "            - generator train times = min(36, max(1, int((avg_realD - avg_fakeD) * 100)) * k)\n",
    "            - 大きい音符が多めの印象\n",
    "            - k=2だとまだ安定してない感じだったので4でやってみた\n",
    "            - まあ遊べるが，ちょっと細かい音符が多かったり不安定\n",
    "            - 500epochs学習させてみたら割と安定してきた．\n",
    "                - 16分未満の音符を削除するだけで割とちゃんとメロディになった\n",
    "                - したがって，音符の切れ目を表現することができた\n",
    "                - いや，down resolution後のステップの終端が空白ならそれを音符の切れ目とすることで表現できた\n",
    "                - メロディにはなるんだけど，やっぱり繰り返しとかが無くて名曲にはなりにくい\n",
    "            - a gttの上限を固定すると，だんだんその上限じゃ足りなくなってきて，最終的にどんなに学習してもDに勝てなくなる\n",
    "            - 終わりがドなら何でもいいような気がしてきたので，自動生成する小節を指定し，終端をドにしたりできるようにした\n",
    "    - というわけで多分この辺が限界なのでそろそろWGAN-gpやSAGANの勉強を始める"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "リファクタリング後の関数軍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sample(G, sample_noise, prev, chord, epoch, directory):\n",
    "    sample_size = len(sample_noise)\n",
    "    sample_fake = G(sample_noise, prev[:sample_size], chord[:sample_size]).detach()\n",
    "    _, _, steps, pitch_range = sample_fake.shape\n",
    "    sample_fake = sample_fake.reshape(sample_size*steps, pitch_range).T\n",
    "    fake_image_path = os.path.join(directory, f'fake_samples_epoch{epoch:03}.png')\n",
    "    vutils.save_image(sample_fake, fake_image_path, normalize=True)\n",
    "    print(f\"{fake_image_path} saved.\")\n",
    "\n",
    "def save_model(G, D, epoch, directory):\n",
    "    hyper_param_str = f\"epoch={epoch:04}_z={G.z_dim}\"\n",
    "    G_file_path = os.path.join(directory, f'G_{hyper_param_str}.pth')\n",
    "    D_file_path = os.path.join(directory, f'D_{hyper_param_str}.pth')\n",
    "    torch.save(G.module.state_dict() if type(G) is MultiGPUWrapper else G.state_dict(), G_file_path)\n",
    "    torch.save(D.module.state_dict() if type(D) is MultiGPUWrapper else D.state_dict(), D_file_path)\n",
    "    print(\"Following models are saved.\")\n",
    "    print(G_file_path)\n",
    "    print(D_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan_adaptive(G, D, g_optim, d_optim, k, max_gtt, fm_lambda, dataloader, epoch_num, output_dir, sample_noise, device,\n",
    "              start_epoch=1, clear_disp_epoch_period=3, save_epoch_period=None, get_status_dict=True):\n",
    "    \n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    batch_num = len(dataloader)\n",
    "    batch_size = dataloader.batch_size\n",
    "    data_size = len(dataloader.dataset) // batch_size\n",
    "    \n",
    "    status_dict = { \n",
    "        'd_loss': [], 'd_ans_real': [], 'd_ans_fake': [], \n",
    "        'g_loss': [], 'g_loss_fake': [], 'g_loss_fm': [], 'gtt': [] \n",
    "    }\n",
    "    \n",
    "    G.train()\n",
    "    D.train()\n",
    "\n",
    "    try:        \n",
    "        for epoch in range(start_epoch, epoch_num+1):\n",
    "            print(f\"----- Epoch {epoch:>3} / {epoch_num:<3} start -----\")\n",
    "            t_epoch_start = time.time()\n",
    "            \n",
    "            # adaptive gtt: 本物を本物と識別できる奴に偽物だと見破られるほど生成器が頑張る\n",
    "            if epoch != start_epoch:\n",
    "                avg_real = total_d_ans_real / data_size\n",
    "                avg_fake = total_d_ans_fake / data_size\n",
    "                gtt = min(max_gtt, max(1, int((avg_real - avg_fake) * 100)) * k)\n",
    "            else:\n",
    "                gtt = k\n",
    "            print(f\"generator train times: {gtt}\")\n",
    "            \n",
    "            total_d_loss = 0.0\n",
    "            total_d_ans_real = 0.0\n",
    "            total_d_ans_fake = 0.0\n",
    "            total_g_loss = 0.0\n",
    "            total_g_loss_fake = 0.0\n",
    "            total_g_loss_fm = 0.0\n",
    "            \n",
    "            for batch_iteration, (x, prev_x, y) in enumerate(dataloader, 1):\n",
    "                real, prev_x, y = [item.to(device) for item in [x, prev_x, y]]\n",
    "                \n",
    "                batch_size = real.size()[0]\n",
    "                \n",
    "                ############\n",
    "                # DのTurn\n",
    "                ############\n",
    "                D.zero_grad()\n",
    "\n",
    "                # realデータに対するLoss\n",
    "                d_ans_real, d_logits_real = D(real, prev_x, y)\n",
    "                d_real_label = torch.rand_like(d_logits_real) * 0.5 + 0.7\n",
    "                d_loss_real = nn.BCEWithLogitsLoss()(d_logits_real, d_real_label)\n",
    "\n",
    "                # fakeデータに対するLoss\n",
    "                z = torch.randn(len(prev_x), G.z_dim).to(device)\n",
    "                fake = G(z, prev_x, y)\n",
    "                d_ans_fake, d_logits_fake = D(fake, prev_x, y)\n",
    "                d_fake_label = torch.rand_like(d_logits_fake) * 0.3\n",
    "                d_loss_fake = nn.BCEWithLogitsLoss()(d_logits_fake, d_fake_label)\n",
    "\n",
    "                # DのLoss全体\n",
    "                d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "                # Dのパラメータを更新\n",
    "                d_loss.backward()\n",
    "                d_optim.step()\n",
    "\n",
    "                # Dに関する合計の記録\n",
    "                total_d_ans_real += d_ans_real.mean().item()\n",
    "                total_d_ans_fake += d_ans_fake.mean().item()\n",
    "                total_d_loss += d_loss.item()\n",
    "                \n",
    "                ############\n",
    "                # GのTurn\n",
    "                ############\n",
    "                for t in range(gtt):\n",
    "                    G.zero_grad()\n",
    "\n",
    "                    z = torch.randn(len(prev_x), G.z_dim).to(device)\n",
    "                    fake = G(z, prev_x, y)\n",
    "                    d_fake, d_logits_fake = D(fake, prev_x, y)\n",
    "                    deceive_label = torch.ones_like(d_logits_fake)\n",
    "                    g_loss_fake = nn.BCEWithLogitsLoss()(d_logits_fake, deceive_label)\n",
    "                    \n",
    "                    # Gの特徴マッチング：Gがrealに近いデータを生成できるようにする\n",
    "                    # fakeデータとrealデータの違いが大きいほどペナルティを与える\n",
    "                    mean_g = torch.mean(fake, 0) # fakeデータの平均値\n",
    "                    mean_r = torch.mean(real, 0)   # realデータの平均値\n",
    "                    g_loss_fm = nn.MSELoss(reduction='sum')(mean_g, mean_r) / 2\n",
    "                    g_loss_fm = torch.mul(g_loss_fm, fm_lambda)\n",
    "                    \n",
    "                    # GのLoss\n",
    "                    g_loss = g_loss_fake + g_loss_fm\n",
    "                    \n",
    "                    # Gのパラメータを更新\n",
    "                    g_loss.backward()\n",
    "                    g_optim.step()\n",
    "                \n",
    "                # Gに関する合計の記録\n",
    "                total_g_loss_fake += g_loss_fake.item()\n",
    "                total_g_loss_fm += g_loss_fm.item()\n",
    "                total_g_loss += g_loss.item()\n",
    "\n",
    "            ################\n",
    "            # EpochのReport\n",
    "            ################\n",
    "            \n",
    "            epoch_duration = time.time() - t_epoch_start\n",
    "            remain_sec = epoch_duration * (epoch_num - epoch)\n",
    "            print(f\"Epoch {epoch:>3} / {epoch_num:<3} finished in {epoch_duration:.4f}[sec]\")\n",
    "            print(f\"Remaining Time | {remain_sec/3600:.4f} [hour] | {remain_sec/60:.2f} [min] | {remain_sec:.0f} [sec]\")\n",
    "            \n",
    "            status_dict['d_loss'].append(total_d_loss / data_size)\n",
    "            status_dict['d_ans_real'].append(total_d_ans_real / data_size)\n",
    "            status_dict['d_ans_fake'].append(total_d_ans_fake / data_size)\n",
    "            status_dict['g_loss'].append(total_g_loss / data_size)\n",
    "            status_dict['g_loss_fake'].append(total_g_loss_fake / data_size)\n",
    "            status_dict['g_loss_fm'].append(total_g_loss_fm / data_size)\n",
    "            status_dict['gtt'].append(gtt)\n",
    "            \n",
    "            last_d_loss = status_dict['d_loss'][-1]\n",
    "            last_d_ans_real = status_dict['d_ans_real'][-1]\n",
    "            last_d_ans_fake = status_dict['d_ans_fake'][-1]\n",
    "            last_g_loss = status_dict['g_loss'][-1]\n",
    "            last_g_loss_fake = status_dict['g_loss_fake'][-1]\n",
    "            last_g_loss_fm = status_dict['g_loss_fm'][-1]\n",
    "            print(f\"==> D loss: {last_d_loss:.4f} | D ans real: {last_d_ans_real:.4f} | D ans fake: {last_d_ans_fake:.4f}\")\n",
    "            print(f\"==> G loss: {last_g_loss:.4f} | G loss fake: {last_g_loss_fake:.4f} | fm G loss: {last_g_loss_fm:.4f}\")\n",
    "            print(\"\")\n",
    "            \n",
    "            if epoch % clear_disp_epoch_period == 0:\n",
    "                clear_output()\n",
    "            \n",
    "            # チェックポイントモデルの保存\n",
    "            if (save_epoch_period is not None) and (epoch % save_epoch_period == 0):\n",
    "                save_model(G, D, epoch, output_dir)\n",
    "                save_sample(G, sample_noise, prev_x, y, epoch, output_dir)\n",
    "                print(\"\")\n",
    "\n",
    "        print(\"All Learning Finished!\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Keyboard interrupted, but return models.\")\n",
    "        if get_status_dict:\n",
    "            return G, D, status_dict\n",
    "        return G, D\n",
    "    \n",
    "    if get_status_dict:\n",
    "        return G, D, status_dict\n",
    "    return G, D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k, max_gtt = 4, 60\n",
    "fm_lambda = 10\n",
    "\n",
    "epoch_num = 500\n",
    "G_trained, D_trained, status_dict = train_gan_adaptive(\n",
    "    G=G, D=D,\n",
    "    g_optim=g_optim, d_optim=d_optim, \n",
    "    k=k, max_gtt=max_gtt, fm_lambda=fm_lambda,\n",
    "    dataloader=dataloader,\n",
    "    epoch_num=epoch_num,\n",
    "    sample_noise=sample_noize,\n",
    "    output_dir=save_dir,\n",
    "    device=device,\n",
    "    start_epoch=1,\n",
    "    clear_disp_epoch_period=5,\n",
    "    save_epoch_period=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_status_graph_gan_adaptive(status_dict, offset=0, burn_in=0, holizon=False, title=\"Score graphs of GAN\"):\n",
    "    d_loss = status_dict['d_loss']\n",
    "    g_loss = status_dict['g_loss']\n",
    "    \n",
    "    d_ans_real = status_dict['d_ans_real']\n",
    "    d_ans_fake = status_dict['d_ans_fake']\n",
    "    \n",
    "    g_loss_fake = status_dict['g_loss_fake']\n",
    "    g_loss_fm = status_dict['g_loss_fm']\n",
    "    \n",
    "    gtt = status_dict['gtt']\n",
    "    \n",
    "    if holizon:\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(24, 6))\n",
    "    else:\n",
    "        fig, axs = plt.subplots(4, 1, figsize=(6, 24))\n",
    "    \n",
    "    x1, x2 = np.arange(len(d_loss))+offset, np.arange(len(g_loss))+offset\n",
    "    axs[0].plot(x1[burn_in:], d_loss[burn_in:], label='D loss', linewidth=1.5)\n",
    "    axs[0].plot(x2[burn_in:], g_loss[burn_in:], label='G loss', linewidth=1.5)\n",
    "    axs[0].legend(loc='upper right')\n",
    "    axs[0].set_xlabel('epoch')\n",
    "    axs[0].set_ylabel('loss')\n",
    "    axs[0].set_title(f\"Losses\")\n",
    "\n",
    "    axs[1].plot(x1[burn_in:], d_ans_real[burn_in:], label='real', linewidth=1.5)\n",
    "    axs[1].plot(x1[burn_in:], d_ans_fake[burn_in:], label='fake', linewidth=1.5)\n",
    "    axs[1].legend(loc='upper right')\n",
    "    axs[1].set_xlabel('epoch')\n",
    "    axs[1].set_ylabel('ans rate')\n",
    "    axs[1].set_title(f\"D ans rates\")\n",
    "    \n",
    "    axs[2].plot(x2[burn_in:], g_loss_fake[burn_in:], label='fake', linewidth=1.5)\n",
    "    axs[2].plot(x2[burn_in:], g_loss_fm[burn_in:], label='fm', linewidth=1.5)\n",
    "    axs[2].legend(loc='upper right')\n",
    "    axs[2].set_xlabel('epoch')\n",
    "    axs[2].set_ylabel('loss')\n",
    "    axs[2].set_title(f\"G break down\")\n",
    "    \n",
    "    x = np.arange(len(gtt)) + offset\n",
    "    axs[3].plot(x, gtt, label='gtt', linewidth=1.5)\n",
    "    axs[3].legend(loc='upper right')\n",
    "    axs[3].set_xlabel('epoch')\n",
    "    axs[3].set_ylabel('gtt')\n",
    "    axs[3].set_title(f\"G train times\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_status_graph_gan_adaptive(status_dict, holizon=True, burn_in=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
