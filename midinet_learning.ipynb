{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MidiNetモデルの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DataLoaderの作成\n",
    "- Modelの作成\n",
    "- 学習コードの作成\n",
    "- 学習経過の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, ipdb, pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from pypianoroll import Multitrack, Track\n",
    "from utils import grid_plot, Timer\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"../datasets/theorytab/midinet\"\n",
    "output_dir = f\"{base_dir}/learning\"\n",
    "\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaderの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MidinetDataloader():\n",
    "    def __init__(self, data_path, pitch_range=[0,128], show_shape=False):\n",
    "        data = pickle.load(open(data_path,'rb'))\n",
    "        \n",
    "        melody, prev, chord = [], [], []\n",
    "        for m, p, c in data:\n",
    "            melody.append(m)\n",
    "            prev.append(p)\n",
    "            chord.append(c)\n",
    "        \n",
    "        self.size = len(melody)\n",
    "        steps = len(melody[0])\n",
    "        bottom, top = pitch_range\n",
    "        \n",
    "        melody = np.array(melody)[:,:,bottom:top].reshape(self.size, 1, steps, top-bottom)\n",
    "        prev = np.array(prev)[:,:,bottom:top].reshape(self.size, 1, steps, top-bottom)\n",
    "        chord = np.array(chord)\n",
    "        \n",
    "        if show_shape:\n",
    "            print(\"melody shape\", melody.shape)\n",
    "            print(\"prev shape\", prev.shape)\n",
    "            print(\"chord shape\", chord.shape)\n",
    "        \n",
    "        self.x = torch.from_numpy(melody).float()\n",
    "        self.prev_x   = torch.from_numpy(prev).float()\n",
    "        self.y  = torch.from_numpy(chord).float()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.prev_x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(data_path, batch_size=72, shuffle=True):\n",
    "    iterator = MidinetDataloader(data_path)\n",
    "    kwargs = {'num_workers': 4, 'pin_memory': True}\n",
    "    data_loader = DataLoader(iterator, batch_size=batch_size, shuffle=shuffle, **kwargs)\n",
    "    print('Data loading is completed.')\n",
    "    print(f'{len(data_loader)} batches from {len(iterator)} bars are obtained.')\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(data):\n",
    "    if not isinstance(data, torch.Tensor):\n",
    "        data = torch.from_numpy(data)\n",
    "    if torch.cuda.is_available():\n",
    "        return data.cuda()\n",
    "    return data.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model用共通関数の作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_vector(x, y):\n",
    "    x_0, _, x_2, x_3 = x.shape\n",
    "    y2 = y.expand(x_0, y.shape[1], x_2, x_3)\n",
    "    return torch.cat((x, y2),1)\n",
    "    \n",
    "\n",
    "def batch_norm(x, eps=1e-05, momentum=0.9, affine=True):\n",
    "    if x.ndim == 2:\n",
    "        return nn.BatchNorm1d(x.shape[1], eps=eps, momentum=momentum, affine=affine).cuda()(x)\n",
    "    elif x.ndim == 3:\n",
    "        return nn.BatchNorm2d(x.shape[1], eps=eps, momentum=momentum, affine=affine).cuda()(x)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "def lrelu(x, leak=0.2):\n",
    "    z = torch.mul(x,leak)\n",
    "    return torch.max(x, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator\n",
    "forwardの入力\n",
    "- z (batch, noise_size) = (72, 113): ランダムノイズ\n",
    "- prev_x (batch, ch, steps, pitch) = (72, 1, 16, 128): 前の小節\n",
    "- y (batch, 13): コード，0~11次元はコードの主音，12次元目はmajorかminorかを区別する\n",
    "\n",
    "forwardの出力\n",
    "- g_x (batch, ch, steps, pitch)= (72, 1, 16, 128): 生成された今の小節\n",
    "    \n",
    "オリジナルのmidinetと同じ．詳しい説明はmidinet_understandingを参照"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,pitch_range=128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gf_dim  = 64\n",
    "        self.y_dim   = 13\n",
    "\n",
    "        self.h1      = nn.ConvTranspose2d(in_channels=157, out_channels=pitch_range, kernel_size=(2,1), stride=(2,2))\n",
    "        self.h2      = nn.ConvTranspose2d(in_channels=157, out_channels=pitch_range, kernel_size=(2,1), stride=(2,2))\n",
    "        self.h3      = nn.ConvTranspose2d(in_channels=157, out_channels=pitch_range, kernel_size=(2,1), stride=(2,2))\n",
    "        self.h4      = nn.ConvTranspose2d(in_channels=157, out_channels=1, kernel_size=(1,pitch_range), stride=(1,2))\n",
    "\n",
    "        self.h0_prev = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(1,pitch_range), stride=(1,2))\n",
    "        self.h1_prev = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(2,1), stride=(2,2))\n",
    "        self.h2_prev = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(2,1), stride=(2,2))\n",
    "        self.h3_prev = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(2,1), stride=(2,2))\n",
    "\n",
    "        self.linear1 = nn.Linear(113,1024)\n",
    "        self.linear2 = nn.Linear(1037,self.gf_dim*2*2*1)\n",
    "\n",
    "    def forward(self, z, prev_x, y, batch_size):\n",
    "        \n",
    "        h0_prev = lrelu(batch_norm(self.h0_prev(prev_x)))   # 72, 16, 16, 1\n",
    "        h1_prev = lrelu(batch_norm(self.h1_prev(h0_prev)))  # 72, 16, 8, 1\n",
    "        h2_prev = lrelu(batch_norm(self.h2_prev(h1_prev)))  # 72, 16, 4, 1\n",
    "        h3_prev = lrelu(batch_norm(self.h3_prev(h2_prev)))  # 72, 16, 2, 1\n",
    "\n",
    "        yb = y.view(batch_size, self.y_dim, 1, 1)           # 72, 13, 1, 1\n",
    "\n",
    "        z = torch.cat((z,y),1)                              # 72, 113\n",
    "\n",
    "        h0 = F.relu(batch_norm(self.linear1(z)))            # 72, 1024\n",
    "        h0 = torch.cat((h0,y),1)                            # 72, 1037\n",
    "\n",
    "        h1 = F.relu(batch_norm(self.linear2(h0)))           # 72, 256\n",
    "        h1 = h1.view(batch_size, self.gf_dim * 2, 2, 1)     # 72, 128, 2, 1\n",
    "        h1 = concat_vector(h1, yb)                          # 72, 141, 2, 1\n",
    "        h1 = concat_vector(h1, h3_prev)                     # 72, 157, 2, 1\n",
    "\n",
    "        h2 = F.relu(batch_norm(self.h1(h1)))                # 72, 128, 4, 1\n",
    "        h2 = concat_vector(h2, yb)                          # 72, 141, 4, 1\n",
    "        h2 = concat_vector(h2, h2_prev)                     # 72, 157, 4, 1\n",
    "\n",
    "        h3 = F.relu(batch_norm(self.h2(h2)))                # 72, 128, 8, 1 \n",
    "        h3 = concat_vector(h3, yb)                          # 72, 141, 8, 1\n",
    "        h3 = concat_vector(h3, h1_prev)                     # 72, 157, 8, 1\n",
    "\n",
    "        h4 = F.relu(batch_norm(self.h3(h3)))                # 72, 128, 16, 1\n",
    "        h4 = concat_vector(h4, yb)                          # 72, 141, 16, 1\n",
    "        h4 = concat_vector(h4, h0_prev)                     # 72, 157, 16, 1\n",
    "\n",
    "        g_x = torch.sigmoid(self.h4(h4))                    # 72, 1, 16, 128\n",
    "\n",
    "        return g_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator\n",
    "forwardの入力\n",
    "- x (batch, 1, steps, pitch) = (72, 1, 16, 128): real/fake判定を行う小節データ\n",
    "- y (batch, 13) = (72, 13): コード\n",
    "\n",
    "forwardの出力\n",
    "- h3_sigmoid (batch, 1) = (72, 1): 0~1に押し込められたreal/fake判定結果．0はfake, 1はreal\n",
    "- h3 (batch, 1) = (72, 1): 0~1に押し込められていないreal/fake判定結果\n",
    "- fm (batch, 1+13, steps, pitch) = (72, 14, 16, 128): 特徴マップ．\n",
    "\n",
    "オリジナルのmidinetと同じ．詳しい説明はmidinet_understandingを参照"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,pitch_range=128):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.df_dim = 64\n",
    "        self.dfc_dim = 1024\n",
    "        self.y_dim = 13\n",
    "        \n",
    "        # out channels = y_dim +1 \n",
    "        self.h0_prev = nn.Conv2d(in_channels=14, out_channels=14, kernel_size=(2,pitch_range), stride=(2,2))\n",
    "\n",
    "        # out channels = df_dim + y_dim\n",
    "        self.h1_prev = nn.Conv2d(in_channels=27, out_channels=77, kernel_size=(4,1), stride=(2,2))\n",
    "        self.linear1 = nn.Linear(244, self.dfc_dim)\n",
    "        self.linear2 = nn.Linear(1037, 1)\n",
    "\n",
    "    def forward(self, x, y, batch_size):        \n",
    "\n",
    "        yb = y.view(batch_size,self.y_dim, 1, 1)\n",
    "        x = concat_vector(x, yb)                    #72, 14, 16, 128\n",
    "        \n",
    "        h0 = lrelu(self.h0_prev(x))                 #72, 14, 8, 1\n",
    "        fm = h0\n",
    "        h0 = concat_vector(h0, yb)                  #72, 27, 8, 1\n",
    "\n",
    "        h1 = lrelu(batch_norm(self.h1_prev(h0)))    #72, 77, 3, 1\n",
    "        h1 = h1.view(batch_size, -1)                #72, 231\n",
    "        h1 = torch.cat((h1,y), 1)                   #72, 244\n",
    "\n",
    "        h2 = lrelu(batch_norm(self.linear1(h1)))\n",
    "        h2 = torch.cat((h2,y), 1)                   #72, 1037\n",
    "\n",
    "        h3 = self.linear2(h2)\n",
    "        h3_sigmoid = torch.sigmoid(h3)\n",
    "\n",
    "\n",
    "        return h3_sigmoid, h3, fm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習コードの作成: original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = os.path.join(base_dir, \"midinet_original.pkl\")\n",
    "save_dir = os.path.join(output_dir, \"original\")\n",
    "\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "save_npa = lambda file_name, npa: np.save(os.path.join(save_dir, file_name), npa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ハイパーパラメータの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "batch_size = 9192\n",
    "generator_train_times = 4\n",
    "sample_bar_num = 16 # あとで実装\n",
    "\n",
    "# for Adam\n",
    "lr = 0.0002\n",
    "betas = (0.5, 0.999)\n",
    "\n",
    "# noise vector size\n",
    "nz = 100\n",
    "\n",
    "# a coefficient to real label for discriminator. 0 ~ 1\n",
    "real_data_worthiness = 0.9\n",
    "\n",
    "# feature matching coefficients\n",
    "lambda_1, lambda_2 = 0.1, 0.01 # D, G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習初期化処理  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading is completed.\n",
      "16 batches from 142740 bars are obtained.\n"
     ]
    }
   ],
   "source": [
    "data_loader = get_dataloader(input_file_path, batch_size=batch_size)\n",
    "data_size = len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "netD = Discriminator()\n",
    "netG = Generator()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    netD = netD.cuda()\n",
    "    netG = netG.cuda()\n",
    "\n",
    "netD.train()\n",
    "netG.train()\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=betas)\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "noise_for_sample = to_device(torch.randn(sample_bar_num, nz))\n",
    "\n",
    "realD_list, fakeD_list = [], [] # Dのrealデータとfakeデータに対するエポックごとの識別結果平均(realに対しては1に近く，fakeに対しては0に近い方がDが強い)\n",
    "lossD_list, lossG_list = [], [] # D, Gのロスのエポックごとの誤差\n",
    "saved_image_paths = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習ループ\n",
    "オリジナルのコードを若干書き換え  \n",
    "ノイズベクトルは毎回作り直すことにしてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 / 200 result\n",
      "==> avg lossD: 0.0001536546 avg lossG: 0.0000902868, avg realD: 0.0000505865, avg fakeD: 0.0000501961 \n",
      "13.796407\n",
      "2.239088\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a484166b7f0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0;31m# Gにノイズベクトル，前の譜面，コードを渡し，fakeデータを作成\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                 \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                 \u001b[0mfake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-7b683879550a>\u001b[0m in \u001b[0;36mto_device\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"start leaning...\")\n",
    "for epoch in range(1, epochs+1):\n",
    "    sum_lossD, sum_lossG = 0, 0\n",
    "    sum_realD, sum_fakeD = 0, 0\n",
    "    \n",
    "    with Timer():\n",
    "        for i, (real, prev, chord) in enumerate(data_loader):\n",
    "\n",
    "            # バッチ(譜面，前の譜面，コード)をdeviceに渡す  \n",
    "            real, prev, chord = [to_device(item) for item in [real, prev, chord]]\n",
    "\n",
    "            # batchの切れ端はサイズが異なる場合があるので注意\n",
    "            batch_size = real.size(0)\n",
    "\n",
    "            ############################\n",
    "            # Dの学習: log(D(x)) + log(1 - D(G(z))) を最大化\n",
    "            # realデータを1，fakeデータを0と判断させるよう学習\n",
    "            ###########################\n",
    "\n",
    "            # Dの勾配の初期化\n",
    "            netD.zero_grad()\n",
    "\n",
    "            # realに対する識別結果からクロスエントロピー誤差(目的関数)の値を得る\n",
    "            d_real, d_logits_real, fm_real = netD(real, chord, batch_size)\n",
    "            d_real_label = real_data_worthiness * torch.ones_like(d_real)\n",
    "            d_loss_real = nn.BCEWithLogitsLoss()(d_logits_real, d_real_label)\n",
    "\n",
    "            # Gにノイズベクトル，前の譜面，コードを渡し，fakeデータを作成\n",
    "            noise = to_device(torch.randn(batch_size, nz))\n",
    "            fake = netG(noise, prev, chord, batch_size)\n",
    "\n",
    "            # fakeに対する識別結果からクロスエントロピー誤差(目的関数)の値を得る\n",
    "            d_fake, d_logits_fake, fm_fake = netD(fake.detach(), chord, batch_size)\n",
    "            d_fake_label = torch.zeros_like(d_fake)\n",
    "            d_loss_fake = nn.BCEWithLogitsLoss()(d_logits_fake, d_fake_label)\n",
    "\n",
    "            # 誤差逆伝搬により勾配を更新し，それに基づきDのパラメータを更新する\n",
    "            lossD = d_loss_real + d_loss_fake\n",
    "            lossD.backward(retain_graph=True)\n",
    "            optimizerD.step()\n",
    "\n",
    "            # 学習記録\n",
    "            # real, fakeデータに対してそれぞれrealだと識別した割合\n",
    "            realD, fakeD = d_real.mean().item(), d_fake.mean().item()\n",
    "            sum_realD += realD\n",
    "            sum_fakeD += fakeD\n",
    "            sum_lossD += lossD.item() # Dの学習におけるLoss\n",
    "\n",
    "\n",
    "            ############################\n",
    "            # Gの学習 : log(D(G(z)))を最大化\n",
    "            # fakeデータを1と判断させるよう学習\n",
    "            ###########################\n",
    "\n",
    "            for t in range(generator_train_times):\n",
    "\n",
    "                # Gの勾配の初期化\n",
    "                netG.zero_grad()\n",
    "\n",
    "                # Gにノイズベクトル，前の譜面，コードを渡し，fakeデータを作成\n",
    "                noise = to_device(torch.randn(batch_size, nz))\n",
    "                fake = netG(noise, prev, chord, batch_size)\n",
    "                \n",
    "                # fakeに対して1をラベルとした識別結果からクロスエントロピー誤差(目的関数)の値を得てGの誤差とする\n",
    "                d_fake, d_logits_fake, fm_fake = netD(fake, chord, batch_size)\n",
    "                deceive_label = torch.ones_like(d_fake)\n",
    "                g_loss = nn.BCEWithLogitsLoss()(d_logits_fake, deceive_label) # (72, 1), (72, 1) => scalar tensor\n",
    "\n",
    "                # Dの特徴マッチング：realとfakeでnetDの初段のreluの出力が近くなるようにする\n",
    "                features_from_g = torch.mean(fm_fake, 0) # fakeデータに対するDのfeatureの平均値\n",
    "                features_from_i = torch.mean(fm_real, 0) # realデータに対するDのfeatureの平均値\n",
    "                # fakeとrealの出すfeatureの違いが大きいほどペナルティを与える\n",
    "                fm_g_loss1 = nn.MSELoss(reduction='sum')(features_from_g, features_from_i) / 2\n",
    "                fm_g_loss1 = torch.mul(fm_g_loss1, lambda_1)\n",
    "\n",
    "                # Gの特徴マッチング：Gがrealに近いデータを生成できるようにする\n",
    "                mean_image_from_g = torch.mean(fake, 0) # fakeデータの平均値\n",
    "                mean_image_from_i = torch.mean(real, 0) # realデータの平均値\n",
    "                # fakeデータとrealデータの違いが大きいほどペナルティを与える\n",
    "                fm_g_loss2 = nn.MSELoss(reduction='sum')(mean_image_from_g, mean_image_from_i) / 2\n",
    "                fm_g_loss2 = torch.mul(fm_g_loss2, lambda_2)\n",
    "\n",
    "                # 誤差逆伝搬により勾配を更新し，それに基づきGのパラメータを更新する\n",
    "                lossG = g_loss + fm_g_loss1 + fm_g_loss2\n",
    "                lossG.backward(retain_graph=(t < generator_train_times - 1)) # 最後は計算グラフを放棄\n",
    "                optimizerG.step()\n",
    "\n",
    "            # 学習記録\n",
    "            sum_lossG += lossG.item() # Gの学習におけるLoss\n",
    "    \n",
    "        clear_output()\n",
    "        print(f\"epoch {epoch} / {epochs} result\")\n",
    "\n",
    "        # エポックごとの識別と誤差の記録\n",
    "        realD_list.append(sum_realD / data_size)\n",
    "        fakeD_list.append(sum_fakeD / data_size)\n",
    "        lossD_list.append(sum_lossD / data_size)\n",
    "        lossG_list.append(sum_lossG / data_size)\n",
    "        print(f'==> avg lossD: {lossD_list[-1]:.10f} avg lossG: {lossG_list[-1]:.10f}, avg realD: {realD_list[-1]:.10f}, avg fakeD: {fakeD_list[-1]:.10f} ')\n",
    "    \n",
    "    # 5エポックごとに具体的なロスと識別の状況を報告し，生成データを画像で記録\n",
    "    if epoch % 5 == 0:\n",
    "        print(f'[epoch {epoch}] loss D: {lossD:.4f} \\\n",
    "              loss G: {lossG:.4f}={g_loss:.4f}+{fm_g_loss1:.4f}+{fm_g_loss2:.4f} \\\n",
    "              real D: {realD:.4f} fake D: {fakeD:.4f}')\n",
    "\n",
    "        sample_fake = netG(noise_for_sample, prev[:sample_bar_num], chord[:sample_bar_num], sample_bar_num).detach()\n",
    "        _, _, steps, pitch_range = sample_fake.shape\n",
    "        sample_fake = sample_fake.reshape(sample_bar_num*steps, pitch_range).T\n",
    "\n",
    "        fake_image_path = os.path.join(save_dir, f'fake_samples_epoch{epoch:03}.png')\n",
    "        saved_image_paths.append(fake_image_path)\n",
    "        vutils.save_image(sample_fake, fake_image_path, normalize=True)\n",
    "\n",
    "print(\"finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルと記録の保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_npa('realD_list.npy', realD_list)\n",
    "save_npa('fakeD_list.npy', fakeD_list)\n",
    "save_npa('lossD_list.npy', lossD_list)\n",
    "save_npa('lossG_list.npy', lossG_list)\n",
    "torch.save(netG.state_dict(), os.path.join(save_dir, f'netG_epoch_{epoch}.pth'))\n",
    "torch.save(netD.state_dict(), os.path.join(save_dir, f'netD_epoch_{epoch}.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成データの画像を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.array(Image.open(saved_image_paths[-1]))\n",
    "fig, ax = plt.subplots(figsize=(21, 7))\n",
    "ax.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 誤差グラフの表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossD_print = np.load(os.path.join(save_dir, 'lossD_list.npy'))\n",
    "lossG_print = np.load(os.path.join(save_dir, 'lossG_list.npy'))\n",
    "\n",
    "length = lossG_print.shape[0]\n",
    "x = np.asarray(np.linspace(0, length-1, length))\n",
    "plt.figure()\n",
    "plt.plot(x, lossD_print,label=' lossD', linewidth=1.5)\n",
    "plt.plot(x, lossG_print,label=' lossG', linewidth=1.5)\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('data')\n",
    "plt.ylabel('loss')\n",
    "plt.savefig(os.path.join(save_dir, f'loss_graph_lr={lr}_epoch={epochs}.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"original\"\n",
    "nz = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "サンプル生成用のGにはbatch_normがない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleGenerator(nn.Module):\n",
    "    def __init__(self,pitch_range=128):\n",
    "        super(SampleGenerator, self).__init__()\n",
    "        self.gf_dim  = 64\n",
    "        self.y_dim   = 13\n",
    "\n",
    "        self.h1      = nn.ConvTranspose2d(in_channels=157, out_channels=pitch_range, kernel_size=(2,1), stride=(2,2))\n",
    "        self.h2      = nn.ConvTranspose2d(in_channels=157, out_channels=pitch_range, kernel_size=(2,1), stride=(2,2))\n",
    "        self.h3      = nn.ConvTranspose2d(in_channels=157, out_channels=pitch_range, kernel_size=(2,1), stride=(2,2))\n",
    "        self.h4      = nn.ConvTranspose2d(in_channels=157, out_channels=1, kernel_size=(1,pitch_range), stride=(1,2))\n",
    "\n",
    "        self.h0_prev = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(1,pitch_range), stride=(1,2))\n",
    "        self.h1_prev = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(2,1), stride=(2,2))\n",
    "        self.h2_prev = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(2,1), stride=(2,2))\n",
    "        self.h3_prev = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(2,1), stride=(2,2))\n",
    "\n",
    "        self.linear1 = nn.Linear(113,1024)\n",
    "        self.linear2 = nn.Linear(1037,self.gf_dim*2*2*1)\n",
    "\n",
    "    def forward(self, z, prev_x, y, batch_size):\n",
    "        h0_prev = lrelu(self.h0_prev(prev_x))\n",
    "        h1_prev = lrelu(self.h1_prev(h0_prev))\n",
    "        h2_prev = lrelu(self.h2_prev(h1_prev))\n",
    "        h3_prev = lrelu(self.h3_prev(h2_prev))\n",
    "\n",
    "        yb = y.view(batch_size, self.y_dim, 1, 1)\n",
    "\n",
    "        z = torch.cat((z,y),1)\n",
    "\n",
    "        h0 = F.relu(self.linear1(z))\n",
    "        h0 = torch.cat((h0,y),1)\n",
    "\n",
    "        h1 = F.relu(self.linear2(h0))\n",
    "        h1 = h1.view(batch_size, self.gf_dim * 2, 2, 1)\n",
    "        h1 = concat_vector(h1, yb)\n",
    "        h1 = concat_vector(h1, h3_prev)\n",
    "\n",
    "        h2 = F.relu(self.h1(h1))\n",
    "        h2 = concat_vector(h2, yb)\n",
    "        h2 = concat_vector(h2, h2_prev)\n",
    "\n",
    "        h3 = F.relu(self.h2(h2))\n",
    "        h3 = concat_vector(h3, yb)\n",
    "        h3 = concat_vector(h3, h1_prev)\n",
    "\n",
    "        h4 = F.relu(self.h3(h3))\n",
    "        h4 = concat_vector(h4, yb)\n",
    "        h4 = concat_vector(h4, h0_prev)\n",
    "\n",
    "        g_x = torch.sigmoid(self.h4(h4))\n",
    "\n",
    "        return g_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = Path(os.path.join(output_dir, version)).glob(\"netG_epoch_*\") # generator\n",
    "path_list = [str(path) for path in model_paths]\n",
    "model_path = np.sort(np.array(path_list))[-1]\n",
    "\n",
    "sampleG = sample_generator()\n",
    "sampleG.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chords = \"C C F G Am Dm G C\"\n",
    "melody = [] #number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(batch_size, nz)\n",
    "\n",
    "# 小節生成ループ\n",
    "for bar in range(n_bars):\n",
    "    z = noise[bar].view(1,nz)\n",
    "    y = chord[bar].view(1,13)\n",
    "\n",
    "    if bar == 0:\n",
    "        # 最初の小節はrealデータを使う\n",
    "        prev = data[0].view(1,1,16,128)\n",
    "    else:\n",
    "        # 2小節目からは前の小節を条件にする\n",
    "        prev = list_song[bar-1].view(1,1,16,128)\n",
    "\n",
    "    # ランダムノイズを基に，前の小節と今のコードを条件として渡して，今の小節を生成\n",
    "    sample = netG(z, prev, y, 1, pitch_range)\n",
    "\n",
    "    # 小節を記録\n",
    "    list_song.append(sample)\n",
    "    list_chord.append(y.numpy())\n",
    "\n",
    "# 生成された曲の保存\n",
    "np.save('output_songs.npy',np.asarray(output_songs))\n",
    "np.save('output_chords.npy',np.asarray(output_chords))\n",
    "\n",
    "print('creation completed, check out what I make!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
